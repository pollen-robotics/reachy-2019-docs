'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/reachy-docs/docs/getting-started/','title':"Getting Started",'content':"Getting Started You just received your Reachy and you want to start using it? You are in the right place!\nThe next sections will guide you in the steps required before turning your robot on. We make a great deal of efforts to make this process as simple and quick as possible. Yet, there are still a few steps to follow which are particularly important.\nSo, grab a cup of coffee (or anything else you like) and let's do it. It should only take a few minutes.\n First, you will setup your robot. Then, you will connect to it. You will launch its first behavior!  "});index.add({'id':1,'href':'/reachy-docs/docs/getting-started/install-your-robot/','title':"Install your robot",'content':"Install your robot Your Reachy is shipped already assembled and configured. It should be a matter of minutes before you start program it.\nBefore turning it on, it's always safer to check if you see anything unusual, in the unlikely event it has been damaged during transport. As an example, here is a box with a black Reachy with a single arm:\nIf you have a doubt, do not hesitate to send us a photo.\nThe antennas, the power supply and other smaller parts should be located on the lower layer in the box.\nAttach the robot on its mount On the back of the robot torso, you will find four spots to attach the robot to a pole. You can also use a specific attach system depending on your configuration.\nAlways make sure the robot is correctly fixed before trying to make it move!\n"});index.add({'id':2,'href':'/reachy-docs/docs/getting-started/connect-to-your-robot/','title':"Connect to Your Robot",'content':"Connect to your robot Everything is already installed ðŸŽ‰ Reachy comes with a Raspberry-Pi 4 dedicated to its control. When you receive your Reachy, the board is pre-installed with all required softwares. So, you don't have to install anything on your computer.\nThis section is meant to:\n let you know how to connect to the Raspberry-Pi, give you a bit more information on how things are actually working.  First, you can find the Raspberry-Pi inside the trunk of the robot, as shown on this picture below. If you need to find or re-flash the SD-card, that's where you will find it.\nThen, if you want to update or re-install the system, we recommend you to rewrite the whole image. You can use etcher to burn the ISO. You will need a SD-card with at least a 16Go capacity. You can find the latest ISO here.\nBackup\nMake sure to save and copy your work as it will be lost during the re-writing!\n The image is based on the Raspbian Buster OS (desktop version). We then installed our own software, mainly a few Python packages.\nInstall on your own machine\nWe provide a pre-install Raspberry-Pi to ensure that all Reachy are shipped ready to be used and with the same configuration. Yet, if you want to use your own computer to control Reachy, or if you want to customize the Raspbian image it is possible. It is a more complex approach and requires knowledge on development environment. Some components are dedicated to the Raspberry Pi (such as the camera and you will have to either use a Raspberry Pi or change the cameras).\n What is left to configure is how you want to access your Reachy. There are a few options. We will present the most common below.\nWork directly on the robot You can directly plug a keyboard, mouse (USB) and display (HDMI) on the back of robot and you should be good to go. You can also plug an ethernet cable.\nLogin access\nThe robot comes with the default login: pi and a default password: reachy.\n You will have access to the Raspbian GUI where you will be able to configure everything you need (WiFi, ssh access, login, etc). Please, refer to the official Raspberry-Pi documentation for more information.\nConnect Reachy to the network You can also work from your own computer and access the robot remotely. To do so, you will first need to connect the robot to your network.\nThere are multiple ways of doing that. If you already know how to connect a Raspberry-Pi, you can follow any procedure that you are familiar with. The only modification from a standard Raspbian is the hostname:Â reachy and the default password:Â reachy.\nOtherwise, we will dive into details below.\nUsing Ethernet You can connect your Reachy using the ethernet socket on the back of the robot. Most configuration should work without any additional setup.\nConfiguring the WiFi Configuring the WiFi on a Raspberry-Pi for the first time can be a bit tricky. That's why we developed a simple web dashboard to help you do that. Yet, this dashboard is hosted on the robot, so you need to connect to it.\nThis can be done:\n by connecting once via ethernet or by connecting a keyboard/screen directly to the robot (see above). or via Reachy hotspot: if the robot does not manage to connect to a known WiFi network it will create its own access point that you can join. A minute or two after booting your robot, you should see a new WiFi network named Reachy-AP. You can join it using the passphrase: Reachy-AP.  Accessing the dashboard Once connected to your robot, you can access the dashboard using your web-browser and connecting to http://reachy.local.\nUsing hostname: Note that to work, we use the mDNS protocol and access the robot via its hostname. This should works without any configuration under Mac OS and GNU/Linux or recent version of Windows 10. If it does not work we recommend you to install Bonjour Print Services for Windows.  Alternatively, you can directly access it using the robot IP address on your network (see with your FAI or IT to get the information). The Find App for smartphone may also help you find it. It is available for Android and iOS. More information is accessible on the Raspberry-Pi official documentation.\nWhen you've found reachy's IP, you can directly access the dashboard on http://192.168.0.42/ where you replace 192.168.0.42 by the found IP.\nSetup a new WiFi using the dashboard On the left column of the dashboard, you should see a WiFi Settings option. When you click on it, you should see something similar to the screenshot below.\nYou can see that it's currently on the Hotspot configuration. You can fill the Add new WiFi form on the right, then click on the UPDATE CONFIG button:\nReboot your robot and it should connect to your freshly setup WiFi. If you come back to the Wifi Settings page, you should now see:\nAccess the robot via ssh You can also access Reachy via ssh.\nUsing the IP\nssh pi@192.168.0.42 Replace 192.168.0.42 with the IP you found.\n Using ZeroConf\nssh pi@reachy.local    Note: for information on how to use ssh on your own machine, please refer to the Raspberry-Pi documentation.\nLogin access\nThe robot comes with the default login: pi and a default password: reachy. Make sure to change the password if you enable remote access! Or even better, only connect using ssh key.\n "});index.add({'id':3,'href':'/reachy-docs/docs/getting-started/check-status/','title':"Check the status of Reachy",'content':"Check the status of Reachy Check the mechanics Once connected, the next step is to check that everything is ok. The first thing to check is that the robot is ok. You should check that the arm are in a correct position, otherwise they may turn to go back to their base position and unplug a wire.\nThe base position of the arm looks like this:\nTODO\nYou should also make sure the head is in a base position, meaning looking in front of it.\nCheck all connections The next step is to actually check all connections. This means that we can find all sensors and motors on the robot. The dashboard will help you do that.\nIf you go to the dashboard on http://reachy.local, it will automatically tries to connect to each part (the right arm, the head and the left arm) and check if it finds all required modules. After waiting a few seconds, you should see appear a green box for each part present in your robot.\nFor instance, here we have a Reachy with a head and a right arm fully working:\nIf you see an orange or red box, this means it has detected an issue. For instance, below you can see a Reachy with a head and both arms. But there is an issue on the left arm.\nYou can see it says \u0026ldquo;module dxl_20 missing\u0026rdquo;. This means that it did not manage to find the dynamixel motor 20, so you should check the wire connecting this motor to see if one is unplug (please refer to the technical specifications sections to find out which motor corresponds to which id).\nIf you encounter such problem, please check the FAQ and if it did not help, please report it to our forum so we can help you.\n"});index.add({'id':4,'href':'/reachy-docs/docs/getting-started/faq/','title':"FAQ",'content':"FAQ Can I run the software on my own computer? Yes, most of it. Our main required dependency is USB to serial communication using the serial library. Yet, you will not have access to the camera or our vision primitive that relies on specific hardware (Raspberry-Pi camera and Google Coral TPU).\nWhere are the installed packages on the Raspberry-Pi? All python packages can be found in $HOME/dev.\nCan I have multiple instances connected to the robot at the same time? No! Only one Python software may access the hardware at the same time. So make sure to close all previous softwares/notebooks before trying to connect.\n"});index.add({'id':5,'href':'/reachy-docs/docs/posts/safety/','title':"Safety First",'content':"Safety first  compliance torque temperature limit initial position speed  "});index.add({'id':6,'href':'/reachy-docs/docs/technical-specifications/','title':"Technical Specifications",'content':"General technical specifications Dimensions: https://tinyurl.com/qk8wuoz\nOverall weight: 7 kg\nI/O power supply: DC 24 V / Max. 3 A\nTemperature:\t0-45Â°C\nInterfaces:\tEthernet / Wifi / RS232 / Modbus / Ethernet TCP/IP\n"});index.add({'id':7,'href':'/reachy-docs/docs/technical-specifications/arm/','title':"Arm",'content':"Reachy's arm specifications Weight repartition  Overall Arm: 1670g Shoulder: 240g Upper arm: 610g Forearm: 590g Gripper: 230g  Maximum payload: 500g\nOf course, this may really vary depending on the holding and duration configuration.\nDegrees of freedom Reachy's arm offers 7 degrees of movement + 1 for the gripper\nRight Arm\n   Motor name Angle limits Motor ID     shoulder_pitch -180, 90 10   shoulder_roll -10, 180 11   arm_yaw -90, 90 12   elbow_pitch -125, 0 13   forearm_yaw -150, 150 14   wrist_pitch -50, 50 15   wrist_roll -45, 45 16   gripper -69, 20 17     Left Arm\n   Motor name Angle limits Motor ID     shoulder_pitch -180, 90 20   shoulder_roll -10, 180 21   arm_yaw -90, 90 22   elbow_pitch 0, 125 23   forearm_yaw -150, 150 24   wrist_pitch -50, 50 25   wrist_roll -45, 45 26   gripper -69, 20 27      Standard version Animated by the following motors:\n 1 Dynamixel MX-106T 3 Dynamixel MX-64AT 1 Dynamixel MX-28AT 2 Dynamixel AX-18A  Performance version Animated by the following motors:\n 3 Dynamixel MX-106T 1 Dynamixel MX-64AT 2 Dynamixel MX-28AT 1 Dynamixel AX-18A  "});index.add({'id':8,'href':'/reachy-docs/docs/technical-specifications/gripper/','title':"Gripper",'content':"Reachy's gripper specifications Animated by 1 Dynamixel AX-18A.\nIncludes a micro load cell 0.78 Kg\n"});index.add({'id':9,'href':'/reachy-docs/docs/technical-specifications/head/','title':"Head",'content':"Reachy's head specifications Reachyâ€™s head features two cameras: one to observe its environment and another camera to focus on the task of manipulating. The head is animated by Orbita, a unique technology developed by Pollen Robotics\u0026rsquo; R\u0026amp;D team. This ball joint actuator allows unpreceded dynamic and multi-directional movement. With animated antennas, Reachy can convey many emotions to his audience.\nCoral development board G950-01456-01\nSee the head in action in this video: https://youtu.be/X9dgsLX_u9I\nCameras 2 Raspberry Pi cameras associated with 2 optical lenses (one macro and one wide angle). See details on Raspberry Pi Camera module here: https://www.raspberrypi.org/documentation/hardware/camera/\nOrbita neck joint Ball joint actuator that is composed of a parallel mechanism motorized by 3 DC Maxon motors. The control of each motor is done with a Pololu magnetic encoder and an LUOS DC motor module.\nAntennas Antennas are animated by a Dynamixel motor and are removable. A system of 3 magnets (2 south and 1 north) allow to attach the antennas to the rotation axis.\n"});index.add({'id':10,'href':'/reachy-docs/docs/technical-specifications/torso/','title':"Torso",'content':"Reachy's torso specifications Reachy's torso area includes the following elements :\nComputer: RASPBERRY PI 4 2G MODEL B\nMicrophone: ReSpeaker Mic Array v2.0\nTPU: Coral G950-01456-01\nSpeaker: Visaton VS-FRS7/8\nAmplifier: Drocking PAM8620\nEmbedded PC\nPower supply: power 180W - output 12V (input 110/120)\n"});index.add({'id':11,'href':'/reachy-docs/docs/program-your-robot/','title':"Program your robot",'content':"Program your robot This section will guide you in how to control your robot. It will describe the most basic features needed to make your robot move and interact with its environment.\nThe robot API is written in Python and versions above 3.6 are supported. It is pre-installed on the Raspberry-Pi of the robot. So to program your Reachy you don't need to install anything on your own machine. Just connect to the Raspberry Pi as described in the Getting Started.\nWorking remotely on the robot is possible via ssh or using a Jupyter server (Jupyter is pre-installed on the Raspberry Pi).  The SDK has been designed to be accessible and easy-to-use. Yet, as it is fully open-source (available here), you can dig inside and adapt it to your specifics needs (contributions are welcome!).\nIn this section, we will cover:\n how to instantiate your robot and define the parts you are using, make the arm moves (motor by motor or using kinematics), recording and replay motions, make the head look somewhere specific, and run pre-defined behaviors.  The full Python's API is also accessible here.\nBefore actually running any code, we strongly recommend you to take a look at the Safety first section. It provides simple guidelines to avoid damaging your robot.  More advanced topics (like using advanced vision for the TicTacToe demo) will be discussed in their own chapter.\n"});index.add({'id':12,'href':'/reachy-docs/docs/program-your-robot/instantiate-your-robot/','title':"Instantiate Your Robot",'content':"Instantiate your robot The first step is to \u0026ldquo;instantiate\u0026rdquo; your robot. What we mean here, is that we will look for the different parts of your robot (connected via USBs on the Raspberry-Pi). We will identify them and check if all modules are connected.\nWe will also launch the synchronisation between the Raspberry-Pi and the different parts of your robot. The sensors value read from the robot will automatically be updated in your Python object. Similarly you will send command to your Robot effector hardware by simply affecting Python variables.\nWhich parts are present on my Reachy? Reachy is built around the concept of modular parts. A Reachy can be composed of:\n a trunk (with all electronics and power supply) one arm (left or right) or both with different kind of end-effectors a head  So, to instantiate your robot we have to specify which parts you want to use. If you are using a \u0026ldquo;full\u0026rdquo; Reachy, ie with both arm equipped with force gripper, and a head; you can run the following Python code on your Raspberry-Pi:\nfrom reachy import Reachy, parts reachy = Reachy( left_arm=parts.LeftArm( io=\u0026#39;/dev/ttyUSB*\u0026#39;, hand=\u0026#39;force_gripper\u0026#39;, ), right_arm=parts.RightArm( io=\u0026#39;/dev/ttyUSB*\u0026#39;, hand=\u0026#39;force_gripper\u0026#39;, ), head=parts.Head( io=\u0026#39;/dev/ttyUSB*\u0026#39;, ), ) And if you have only the right arm and the head:\nfrom reachy import Reachy, parts reachy = Reachy( right_arm=parts.RightArm( io=\u0026#39;/dev/ttyUSB*\u0026#39;, hand=\u0026#39;force_gripper\u0026#39;, ), head=parts.Head( io=\u0026#39;/dev/ttyUSB*\u0026#39;, ), ) If you don't see any error, good news, you are now connected to your Robot and all the parts have been found! ðŸŽ‰ ðŸŽ‰ ðŸŽ‰\nIf it didn't work, check the FAQ or the forum! Your problem is most likely already describe there.  Going deeper: the arm part Let's dive a bit into the details of the code above.\nfrom reachy import Reachy, parts Our API is available through the reachy Python module. This is the main entry point for controlling your robot.\nright_arm=parts.RightArm( io=\u0026#39;/dev/ttyUSB*\u0026#39;, hand=\u0026#39;force_gripper\u0026#39;, ), Here, we specify that we want to add a Right Arm part and it should be found on a USB serial port of type \u0026ldquo;/dev/ttyUSB*\u0026quot;. This is the standard name for the serial port on a Linux system. On other OS the name may differ (e.g. COM* on Windows).\nThen, we specify which types of hand are attached to the arm. In our cases we set it to \u0026ldquo;force_gripper\u0026rdquo;.\nGoing deeper: the head part head=parts.Head( io=\u0026#39;/dev/ttyUSB*\u0026#39;, ), Similarly to the arm, we define the USB port on which we should find the part \u0026ldquo;/dev/ttyUSB*\u0026quot;.\nReachy: putting everything together The Reachy object is mainly a container to regroup the different parts. It also provides you higher level methods, for instance to program a complex motion happening on multiple parts at the same time.\nreachy = Reachy( left_arm=..., right_arm=..., head=..., ) Reachy's modularity: based on Luos technology To permit the parts system in Reachy, we are relying on the Luos technology. This modular system is based around the concept of using a tiny electronic Luos board for each sensors or effectors in your robot. They can be daisy chained and are automatically detected and recognized.\nThose modules are exposed to the Raspberry-Pi using a Luos gate that communicates via USB-serial interface.\nEach part is composed of several Luos modules:\n Arm  a dynamixel module to communicate with the motors a force sensor module for the gripper a gate to communicate with the Raspberry-Pi   Head  three DC motor controllers for Orbita a dynamixel module for the antennas a gate to communicate with the Raspberry-Pi    "});index.add({'id':13,'href':'/reachy-docs/docs/program-your-robot/control-the-arm/','title':"Control The Arm",'content':"Control the arm Once you have instantiated the Reachy object, you are actually connected to your robot. This means that the hardware (sensors and effectors) are synced with their software equivalent. This synchronization loop runs at about 100Hz. In particular, this lets you retrieve the arm(s) state and send commands to make it move.\nBefore actually running some commands, it's important to make sure you are ready:\n Make sure the robot is in a \u0026ldquo;safe\u0026rdquo; state (as shown in the images below for instance). If a wire is blocked or a motor is completly reversed, sending target position or even turning the motor on may result in a violent motion that may damage the robot. The motor are powerful so they can make the whole arm move fluidly. Do not hesitate to first try the move you want to make, with a low speed (we will show how to do that below). When you are sure everything is okay, you can increase the speed again.  More information is available on the Safety first section. It provides simple guidelines to avoid damaging your robot.  In the examples below, we will show examples on a right arm. Using the left arm instead should be straightforward. Always make sure that the different commands suggested below make sense in your context:\n angles from a right arm to a left arm may differ if your robot is attached in front of a desk or can freely move etc.   Everything is setup? It's time to make Reachy move!\nMoving individual motors Reachy's upper arm is composed of 4 motors:\n shoulder_pitch shoulder_roll arm_yaw elbow_pitch  And depending on the hand used, usually 3 or 4 others motors. For the force gripper, you have:\n forearm_yaw wrist_pitch wrist_roll gripper  Each of these motors can be controlled individually.\nHigher-level ways to control the whole arm are shown in the next sections.  They are servo motors that you control in position. You can also control their compliancy or their torque limit. Theses motors are also behaving as sensor, so their present position can also be read at any time.\nTo access a specific motor (here elbow_pitch) in Python code:\nreachy.right_arm.elbow_pitch \u0026gt;\u0026gt;\u0026gt; \u0026lt;DxlMotor \u0026#34;right_arm.elbow_pitch\u0026#34; pos=\u0026#34;-83.209\u0026#34; mode=\u0026#34;stiff\u0026#34;\u0026gt; And to get its present position:\nreachy.right_arm.elbow_pitch.present_position \u0026gt;\u0026gt;\u0026gt; -83.121 The codes above assume that you already instantiated your Reachy, as shown in the section Instantiate Your Robot and assigned it to the reachy variable.  In a more general manner, you can access any motor by using reachy.part_name.motor_name.\nPlease note that the part name can also be nested (e.g. right_arm.hand). All available parts are:\n right_arm right_arm.hand left_arm left_arm.hand head  If one part is not present in your robot, it's value will simply be None.\n For each motor, we have defined angle limits. They correspond to what moves the arm can actually make. For instance, the elbow pitch is limited from 0Â° to 125Â°. The forearm yaw can move from -150Â° to 150Â°.\nThe zero position of each motor is defined so as when all motors are at 0Â°, the arm is straight down along the trunk. For more information on the motor orientation and configuration, please refer to the section Arm coordinate system.\nWhile each motors has angle limits corresponding to what it can do, all combinations of these limits are not reachable! When moving a motor, you always have to take into consideration the configuration of the other motors.  Compliant or stiff The servo motors used in Reachy's arm have two operating modes:\n compliant: the motors is soft and can be freely turned by hand. It cannot be controlled, setting a new target position will have no effect. Yet you can still read the motor position. stiff: the motors is hard and cannot be moved by hand. It can be controlled by setting new target position.  When you turn on your robot, all motors are compliant. You can freely move Reachy's arm and place it in its base position.\nTo make Reachy keep its position and let you control its motors, you need to turn them stiff. To do that, you can use the compliant property.\nFor instance, to make the elbow stiff, run the following code:\nreachy.right_arm.elbow_pitch.compliant = False Now, the elbow should be hard, you cannot move it by hand anymore.\nTo turn it back compliant, simply run:\nreachy.right_arm.elbow_pitch.compliant = True Setting a new target position for a motor Assuming the motor is now stiff, you can now make it move. Once again, make sure the target position you will set corresponds to a reachable position in your configuration.\nTo make our motor move, we will use the goto method. We will define a target position and a move duration. Let's try having our elbow at 90Â° in 2s:\nreachy.right_arm.elbow_pitch.goto( goal_position=90, # in degrees duration=2, # in seconds wait=True, ) When running this code, you should see the motor move.\nThe wait=True option blocks until the move is actually done. You can set it to False and the function will return immediatly.\nBe careful not to have two trajectories running on the same motor in parallel! This could result in an unpredicted behavior (Trajectory are not thread safe).\n Let's try to move another motor, the arm_yaw:\nreachy.right_arm.arm_yaw.goto( goal_position=20, duration=2, wait=True, ) Run this code\u0026hellip; And if you followed this documentation closely, your motor should not have moved\u0026hellip; Can you guess why? Yes, indeed the motor is still compliant. You have to turn it stiff and then run the code above again.\nAs a safety measure, when turning a motor stiff, its goal position is reset. This avoids to directly jump to a new position when you turn the motor stiff. Indeed, the target position is stored inside the motor and could be kept even if you restart your Python script.  goto vs goal_position To control \u0026amp; motor in the examples above we used the goto method. A lower API to control the motor is to directly use the goal_position property.\nYet, you should be careful when doing so, because the motor will try to reach this new goal position as fast as it can. And it is really fast (up until ~600-700 degrees per sec)! A workaround is to also use the moving_speed property to set the maximum speed that the motor can reach.\nreachy.right_arm.elbow_pitch.moving_speed = 50 # in degrees per sec  reachy.right_arm_elbow_pitch.goal_position = 110 # in degrees Yet, in our experience, when using this approach for controlling a motor, it may be hard to follow smoothly complex trajectories and have precise timing. You only set the maximum speed but have no control over the acceleration.\nThe approach used in the goto method differs in a sense that it only using position control (the maximum speed is allowed)\nA full position trajectory profile is actually generated going from the current position to the goal position. This trajectory is then interpolated at a predefined frequency (100Hz) to compute all intermediary target position that should be followed before reaching the final goal position. Depending on the interpolation mode chosen, you can have a better control over speed and acceleration. More details are given below.\nFor instance, the code below will make the elbow_pitch motor follow this curve (assuming the motor was still in 110Â°):\nreachy.right_arm.elbow_pitch.goto( goal_position=80, # in degrees duration=1, # in seconds wait=True, interpolation_mode='minjerk', ) Remember to reset the maximum speed of the elbow_pitch before runing any goto on it. To reset the maximum speed of a motor (using no speed limit) simply set it to 0:\nreachy.right_arm.elbow_pitch.moving_speed = 0   Moving multiple motors at once Most of the time when controlling a robot, you want to move multiple motors at once. While you can do it by running multiple goto and using the option wait=False, there is a simpler way.\nYou can actually use a goto the robot level and specify a list of (motor, target_position). This will create a trajectory for each motor and run them all in parallel.\nTo move both arm and forearm yaw at the same time, you can run:\nreachy.goto( goal_positions={ \u0026#39;right_arm.arm_yaw\u0026#39;: 45, \u0026#39;right_arm.hand.forearm_yaw\u0026#39;: -45, }, duration=2, wait=True, ) Note the use of the complete motor name (eg. \u0026lsquo;right_arm.hand.forearm_yaw\u0026rsquo;) as a key string in the goal_positions dict.  Motors from different parts can be mixed in the same goto.\nreachy.goto( goal_positions={ \u0026#39;head.left_antenna\u0026#39;: 0, \u0026#39;head.right_antenna\u0026#39;: 0, \u0026#39;right_arm.elbow_pitch\u0026#39;: 90, }, duration=2, wait=True, ) You can concatenate multiple goto simply by using the wait=True option and running both codes sequentially:\nreachy.goto( goal_positions={ \u0026#39;right_arm.arm_yaw\u0026#39;: 45, \u0026#39;right_arm.hand.forearm_yaw\u0026#39;: -45, }, duration=2, wait=True, ) reachy.goto( goal_positions={ \u0026#39;right_arm.arm_yaw\u0026#39;: 0, \u0026#39;right_arm.hand.forearm_yaw\u0026#39;: 0, }, duration=1, wait=True, ) You can also define positions, save them and then directly go to those positions.\nfirst_pos = { \u0026#39;right_arm.arm_yaw\u0026#39;: 0, \u0026#39;right_arm.hand.forearm_yaw\u0026#39;: 0, } second_pos = { \u0026#39;right_arm.arm_yaw\u0026#39;: 45, \u0026#39;right_arm.hand.forearm_yaw\u0026#39;: -45, } for _ in range(3): reachy.goto(goal_positions=first_pos, duration=2, wait=True) reachy.goto(goal_positions=second_pos, duration=1, wait=True) An easy way to define a position is to actually record it directly on the robot. To do that, you can play with the compliant mode so you can freely move the robot and make it adopt the position you want. Then, you can record its position using a code similar to:\ncurrent_arm_position = { motor.name: motor.present_position for motor in reachy.right_arm.motors } current_reachy_whole_position = { motor.name: motor.present_position for motor in reachy.motors } Make sure only the motors you want to track are actually included in the position you have recorded.\n Different trajectory interpolation All goto accept a interpolation_mode argument. This lets you define the way you want the trajectory to interpolate from A to B.\nIt cames with two basic way:\n linear interpolation minimum jerk  Running the same goto (from 0Â° to 90Â°) and changing the interpolation mode from linear to minjerk will result in the two different trajectories:\nNote that both trajectories start and finish at the same points. Yet, the followed positions and therefore speed and acceleration differs quite a bit. The Minimum Jerk will slowly accelerate at the begining and slowly decelerate at the end.\nWhat's show in the figure above is a theoretical trajectory. The motor has a low level controller that will try to follow this curve as closely as possible. Yet, depending on the speed/acceleration that you try to reach and the motor configuration, if it has to move a big loads, the real and theoretical curves may differ.  Grasping While the motors of the Force Gripper hand can be controlled like the rest of the motors of the arm, there is an additional functionality that lets you easily close and open the gripper.\nOpening the gripper is as simple as:\nreachy.right_arm.hand.open() This actually simply runs a goto on the gripper motor with pre-defined target position (-30Â°) and duration (1s).\nMore interestingly, you can also use the close method:\nreachy.right_arm.hand.close() This also runs a goto on the gripper, but it uses the force sensor inside the gripper to detect when it did grab something. It will try to automatically adjust the grip, to maintain enough force to hold the object while not forcing too much and overheat the motor.\nAll parameters used in the close method can be adjusted depending on your needs (please refer to the the APIs for more details).\nThe gripper on Reachy's end effector is not meant to hold objects for a long time. The motor used in the gripper will quickly overload if doing so. Holding objects as you can see on the TicTacToe demo for instance is a good approximation of what the robot can do for long period of time (hold ~5s rest ~10s).\nIf you need to hold objects for longer period of time, you probably need to have a specific gripper for this task. While this is part of Pollen Robotics plan, we do not yet have a ready-to-use solution for this kind of use. Let us know if you would be interested in such features or want to help design one.\n The Force Gripper also gives you access to the current grip_force. The load sensor is located inside Reachy's gripper. It measures the force applied on the gripper (exactly it measures the deformation of the gripper due to the grasping).\nprint(reachy.right_arm.hand.grip_force) \u0026gt;\u0026gt;\u0026gt; 123.1 The returned value is nor accurate nor expressed in a standard unit system. Only relative values should be taken into account.\nFor instance:\nif abs(reachy.right_arm.hand.grip_force) \u0026lt; 50: print(\u0026#39;not holding\u0026#39;) else: print(\u0026#39;holding\u0026#39;)   Record \u0026amp; Replay Trajectories So far, we have showed you how to make your robot move using goto and pre-defined position. This works well for simple motion. But sometimes you would like to have more complex motions. That's when another technique comes into place: recording by demonstration.\nRecord by demonstration With this approach, you will demonstrate whole trajectories on Reachy by moving it by hand (using the compliant mode) and record its position at high-frequency (about 100Hz). Depending on what you want, you can record a single motor, or multiple at the same time. We provide a TrajectoryRecorder object that makes this process really simple.\nFor instance, assuming you want to record a movement on the Right Arm and that it is already compliant:\nrecorder = TrajectoryRecorder(reachy.right_arm.motors) And when you are ready to record:\nrecorder.start() And when the movement is over:\nrecorder.stop() The recorded trajectories can then be accessed via:\nprint(recorder.trajectories) \u0026gt;\u0026gt;\u0026gt; TODO You can save it as a numpy array using:\nimport numpy as np np.savez(\u0026#39;my-recorded-trajectory.npz\u0026#39;, **recorder.trajectories) The recorder can be restart as many times as you want. A new trajectory will be recorded on store as recorder.trajectories.\nYou can record any list of motors. For instance if you want to record the arm yaw and the gripper you can use:\nrecorded_motors = [reachy.right_arm.arm_yaw, reachy.right_arm.hand.gripper] recorder = TrajectoryRecorder(recorded_motors) recorder.start() time.sleep(10) recorder.stop() Replay a trajectory Let's say you have recorded a nice motion and save it on the disk using the code above. You can first reload it at any time using:\nmy_loaded_trajectory = np.load(\u0026#39;my-recorded-trajectory.npz\u0026#39;) Then, you want to play it on the robot. To do that, we have created an object called TrajectoryPlayer. It can be used as follow:\ntrajectory_player = TrajectoryPlayer(reachy, my_loaded_trajectory) First, you need to specify on which robot you want to play the trajectory. If you have multiple Reachy, you can record a movement on one and play it on the other. Then, you need to specify which trajectory you want to play.\nAssuming the motors you want to use to play the trajectory are stiff and that the robot is in position where he can play the trajectory:\ntrajectory_player.play(wait=True, fade_in_duration=1.0) The code above will play the trajectory, wait for it to finish.\nThe fade_in_duration ensure there is no jump at the begining of the motion. Indeed, if you play a trajectory from a different starting point that what you record, the robot will try to reach this starting position as fast as it can. This parameter basically says, \u0026ldquo;first goto the recorded starting position in 1s, then play the trajectory\u0026rdquo;.\nAlways think about starting position before recording and replaying trajectories!\n You can also play multiple trajectories as a sequence. Assuming you have recorded three motion (traj_1, traj_2 and traj_3):\nTrajectoryPlayer(reachy, traj_1).play(wait=True, fade_in_duration=1) TrajectoryPlayer(reachy, traj_2).play(wait=True, fade_in_duration=1) TrajectoryPlayer(reachy, traj_3).play(wait=True, fade_in_duration=1) or in a more concise way:\nfor traj in [traj_1, traj_2, traj_3]: TrajectoryPlayer(reachy, traj).play(wait=True, fade_in_duration=1) Work on a trajectory - Smoothing Recorded trajectories are actually simple objects. It's a dict in the form:\n{motor_name: numpy_array_of_position}\nPosition arrays have the same length for all motors. You can thus convert the whole trajectory as a 2D array of shape MxS where M is the number of motor and S is the number of sample in the trajectory. The number of sample corresponds roughly to the duration in seconds x 100 (the default sampling frequency).\nBy recording a trajectory by demonstration, you may sometimes also record involuntary movements or jerkiness. When replayed, you may observe such artefacts. A good practice is to actually apply some smoothing/filtering on the trajectory before saving it.\nAs they are store as numpy arrays, you can use all typical libraries for this task (numpy/scipy/etc). Depending on the type of movements you recorded or what you want to do (eg. really smooth and slow moves, accurate trajectory, etc), the smoothing you want to apply may vary and there is not a single approach that will work for all types of moves.\nWe still provide a cubic smooth functionalities as it is something we often use. The cubic smoothing will garantee to have continuous acceleration which is really important for human perception. You simply need to choose the number of keypoints that will be used to represent your smoothed trajectory.\nfrom reachy.trajectory.interpolation import cubic_smooth smoothed_trajectories = cubic_smooth(recorder.trajectories, nb_kp=10) The cubic smooth function will keep the same number of samples by default. But you can modify this behavior using the out_points parameters.  Arm coordinate system Joint coordinates In all examples above, we have used what is called joint coordinates. This means that we have controlled each joint separately.\nKinematic model Controlling a robot in joint coordinates can be hard and is often far from what we actually do as humans. When we want to grasp an object in front of us, we think of where we should put our hand, not how to flex each individual muscle to reach this position. This approach relies on the cartesian coordinates: the 3D position and orientation in space.\nForward and Inverse Kinematics is a way to go from one coordinates system to the other:\n forward kinematics: joint coordinates \u0026ndash;\u0026gt; cartesian coordinates inverse kinematics: cartesian coordinates \u0026ndash;\u0026gt; joint coordinates  We have defined the whole kinematic model of the arm. This means the translation and rotation required to go from one joint to the next one. On a right arm equipped with a force gripper this actually look like this:\n   Motor Translation Rotation     shoulder_pitch (0, -0.19, 0) (0, 1, 0)   shoulder_roll (0, 0, 0) (1, 0, 0)   arm_yaw (0, 0, 0) (0, 0, 1)   elbow_pitch (0, 0, -0.28) (0, 1, 0)   forearm_yaw (0, 0, 0) (0, 0, 1)   wrist_pitch (0, 0, -0.25) (0, 1, 0)   wrist_roll (0, 0, -0.0325) (1, 0, 0)   gripper (0, -0.01, -0.075) (0, 0, 0)    This describe the translation and rotation needed to go from the previous motor to the next one. We actually use a simplified Denavit Hartenberg notation.\nTo use and understand kinematic model, you need to know how Reachy coordinate system is defined (from Reachy's perspective):\n The X axis corresponds to the foward arrow. The Y axis corresponds to the right to left arrow. The Z axis corresponds to the up arrow.  And the origin of this coordinate system is located in the upper part of the robot trunk. Basically, if you imagine a segment going from the left shoulder to the right shoulder of the robot, the origin is the middle of this segment.\nThe units used for this coordinate system are the meter. So the point (0.3, -0.2, 0) is 30cm in front of the origin, 20cm to the right and at the same height.\nForward kinematics Using the joint coordinates and the kinematic model defined above, we can actually compute the 3D position and orientation of any joint in the arm. This question can be formalized as a trigonometry problem and it's called the forward kinematics.\nWe provide a function to directly compute the forward kinematics of a reachy arm. Assuming you are working on a Right Arm with a Force Gripper (8 joints), you can find the pose when all motors are at their zero position:\nprint(reachy.right_arm.forward_kinematics(joints_position=[0, 0, 0, 0, 0, 0, 0, 0])) \u0026gt;\u0026gt;\u0026gt; array([[ 1. , 0. , 0. , 0. ], [ 0. , 1. , 0. , -0.2 ], [ 0. , 0. , 1. , -0.6375 ], [ 0. , 0. , 0. , 1. ]]) The 4x4 matrix returned by the forward kinematics method is what is often called a pose. It actually encodes both the 3D translation (as a 3D vector) and the 3D rotation (as a 3x3 matrix) into one single representation. $$\\begin{bmatrix} R_{11} \u0026amp; R_{12} \u0026amp; R_{13} \u0026amp; T_x\\\\\\\nR_{21} \u0026amp; R_{22} \u0026amp; R_{23} \u0026amp; T_y\\\\\\\nR_{31} \u0026amp; R_{32} \u0026amp; R_{33} \u0026amp; T_z\\\\\\\n0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}$$  You can also get the current pose by doing:\ncurrent_position = [m.present_position for m in reachy.right_arm.motors] print(reachy.right_arm.forward_kinematics(joints_position=current_position)) \u0026gt;\u0026gt;\u0026gt; array([[ 0.001, 0. , -1. , 0.316], [ 0. , 1. , 0. , -0.209], [ 1. , -0. , 0.001, -0.308], [ 0. , 0. , 0. , 1. ]]) If you are using an external 3D tracker, you may observe a difference between your measure of the end effector and the one given by the forward kinematics. Keep in mind that the forward kinematics rely on a theoretical model of the Reachy arm. You may have difference due to motor jerk or assembly approximation.  Inverse kinematics Knowing where you arm is located in the 3D space can be useful but most of the time what you want is to move the arm in cartesian coordinates. You want to have the possibility to say: \u0026ldquo;move your hand to [x, y, z] with a 90Â° rotation around the Z axis\u0026rdquo;.\nThis is what inverse kinematics does. We have provided a method to help you with that. You need to give it a 4x4 target pose (as given as a result by the forward kinematics) and an initial joint state.\nTo make this more concrete, let's first try with a simple example. We will make the hand of the robot goes from a point A to a point B in 3D space. You always have to provide poses to the inverse kinematics that are actually reachable by the robot.\nFor our starting point, let's imagine a point in front of the robot rigth shoulder and slightly below. If you remember Reachy coordinate system, we can define such a point with the following coordinates: $$A=(0.3, -0.2, -0.3)$$\nFor our end point we will stay in a parallel plan in front of the robot (we keep the same x) and move to the upper left (20cm up and 20cm left). This gives us a B point such that:\n$$B=(0.3, 0.0, -0.1)$$\nBut having the 3D position is not enough to design a pose. You also need to provide the 3D orientation. The identity rotation matrix corresponds to the zero position of the robot, ie. when the hand is facing toward the bottom. So if we want the hand facing forward when going from A to B, we need to rotate it from -90Â° around the y axis.\nThe corresponding matrix can be obtained from scipy:\nfrom scipy.spatial.transform import Rotation as R print(R.from_euler(\u0026#39;y\u0026#39;, np.deg2rad(-90)).as_matrix(), 2) \u0026gt;\u0026gt;\u0026gt; array([[ 0., -0., -1.], [ 0., 1., -0.], [ 1., 0., 0.]]) By combining the position and orientation, this give us the following poses:\nA = np.array(( (0, 0, -1, 0.3), (0, 1, 0, -0.2), (1, 0, 0, -0.3), (0, 0, 0, 1), )) B = np.array(( (0, 0, -1, 0.3), (0, 1, 0, 0.0), (1, 0, 0, -0.1), (0, 0, 0, 1), )) Finding the correct poses may be tricky, especially the orientation. You can always use the forward kinematics, with joint positions that corresponds roughly to where you are trying reach, to get a reference position and orientation.\nThe inverse kinematics is also often used in combination with an external tracking system that can provide 3D pose of predefined object. In such case, you only need to make sure to express all pose in the same coordintate system.\n The basic inverse kinematics used for Reachy relies on optimisation techniques. Results will thus largely depend on the initial guess you provide (if you are not supplying one, the current joints position of the robot will be used).\nIn our case, we can imagine that a base position could corresponds to moving the elbow to -90 to face forward, thiss give us the full joint position to be [0, 0, 0, -90, 0, 0, 0, 0].\nIf we put everything together, we can now ask the inverse kinematics to compute the joint position that corresponds to our A and B poses:\nJA = reachy.right_arm.inverse_kinematics(A, q0=[0, 0, 0, -90, 0, 0, 0, 0]) print(np.round(JA, 2)) \u0026gt;\u0026gt;\u0026gt; [3.1, 0.9, 0.82, -94.39, 1.18, 0.43, -0.8, 0.] JB = reachy.right_arm.inverse_kinematics(B, q0=[0, 0, 0, -90, 0, 0, 0, 0]) print(np.round(JB, 2)) \u0026gt;\u0026gt;\u0026gt; [-21.25, 10., 42.65, -110.79, 16.46, 39.25, -28.3, 0.] You can then define a goto_right_arm_joint_solution and use it to go from A to B:\nimport time def goto_right_arm_joint_solution(joint_solution, duration, wait): reachy.goto({ m.name: j for j, m in zip(joint_solution, reachy.right_arm.motors) }, duration=duration, wait=wait) goto_right_arm_joint_solution(JA, duration=2, wait=True) time.sleep(2) goto_right_arm_joint_solution(JB, duration=2, wait=True) You should have seen the robot follow the 3D line that we defined!\nInverse kinematics is a really powerful way to define motions in coordinate systems that fits better with the defintion of many tasks (grasp the bottle in (x, y, z) for instance). Yet, this approach has also some important limitations.\nFirst, it's important to understand that while the forward kinematics has a unique and well defined solution, the inverse kinematics is a much harder and ill defined problem. A Right Arm with a Gripper has 8 Degrees Of Freedom (8 motors) meaning that you may have multiple solutions to reach the same 3D point in space.\nSecond, there are many approaches to solve the inverse kinematics. The main one used in Reachy is based on an black-box optimisation technique that may take time to converge. You also need to give it a starting point that may have a tremendous influence on the final result. Other approaches can be used but they all have they pros and cons and you may use one or the other depending on the task you are trying to solve.\nMotor temperature and security The motor used in Reachy arm are servo-motors and they are providing extra information.\nYou can get their current temperature (in Â°C). For instance, to get the temprature of the elbow_pitch you can simply do:\nprint(reachy.right_arm.elbow_pitch.temperature) \u0026gt;\u0026gt;\u0026gt; 34.1 There is an automatic behavior that monitors those temperature and will turn on the fan inside the arm to cool it down if needed. A temperature above 45Â°C will trigger the fan, while going below 40Â°C will turn it off.\nThere is also an internal safety inside the motor that will automatically shut the motor down if it reaches 55Â°C. This is to avoid damaging a motor. To turn it on again, you will have to power it off and on again. There is also an overload safety. It will also shutdown the motor if the load applied on the motor is too high for a period of time. See the Safety first section for more details.\n"});index.add({'id':14,'href':'/reachy-docs/docs/program-your-robot/control-the-head/','title':"Control The Head",'content':"Control the head The head is an important part of Reachy. It's where its camera are located and it participates a lot to the robot expressivity.\nIn the following sections, you will see how:\n you can orient the head by controlling its neck, to move the antennas, and use its cameras.  In the examples below, we will assume that you have already instantiated the head part of a Reachy as show in the section Instantiate Your Robot.\nHoming Before being able to control the head, there is a required calibration step. The position coders used in the Orbita neck only provides relative positionning. The calibration procedure will automatically find the zero position of the head for you.\nThis calibration, also called homing, is rather simple. All three disks composing orbita will turn in the same direction, making the head turn to the left until they reach their limit. We can used this left physical limits as a known absolute position (-160Â° for all three disks). We then go back to the zero position, meaning when the head is looking straight forward.\nTo do the homing:\n First, make sure the head can freely turn. Then, run the following python line of code:  reachy.head.homing() You should see the head realises the motion described above and end up in its base position. The whole procedure takes about 5 seconds.\nThe head homing must be performed every time you start your robot after powering it off.  Looking around Now that the head has been calibrated, we can start controlling it. We will show you two main ways to control the neck and orient the head:\n Moving each disks individually Specifing look points in space.  The neck has 3 degrees of freedom (one per disk). When combined those 3 rotations will allow you to set a 3D orientation of the head.\nControlling the disks individually Each disk is actuated by a motor that can be controlled in almost the same way than motors from the arm. In particular, you will be able to:\n turn them stiff/compliant, set a new target position, define a maximum speed, read their current position (in degrees), get their current temperature (in Â°C).  It's important to understand how you can control the disks of Reachy's head as it's always how they are control behind the scene. Yet, in the next section we will show you simpler way to control the head orientation and look at specific 3D points in space.  Stiff/Compliant Like for the motors of the arm, the head can be used in two different modes:\n compliant: where you can freely move the head and still read its current position stiff: where the motors are hard and can be controlled in position.  You can change the mode for a disk by running:\nTo make it compliant:\nreachy.head.neck.disk_bottom.compliant = True To make it stiff:\nreachy.head.neck.disk_bottom.compliant = False You can also change the mode for the three disks using:\nfor disk in reachy.head.neck.disks: disk.compliant = True Or you can even use the shortcut:\nreachy.head.compliant = False Control a disk Like the motors of the arm, you can control the disk of the neck by setting new target position. You can also set maximum speed that will be used to reach those targets.\nYou can set a new target position of one disk using:\nTo set the disk_top to 20Â°:\nreachy.head.neck.disk_top.target_rot_position = 20 If the head was stiff, you should have seen of the arm move.\nTo move all three disks at the same time you can run:\nfor disk in reachy.head.neck.disks: disk.target_rot_position = -20 We also provide a goto function that lets you control the move duration as well. For instance to go back to 0 position on all three disks in 1 second:\nreachy.head.neck.goto(thetas=(0, 0, 0), duration=1, wait=True) The goto method supports the same extra arguments than for the arm version:\n wait (True/False): whether or not to wait for the end of the move interpolation_mode: to specify how the trajectory is computed (see Trajectory interpolation for details).  Read positions or temperature The current position of each disk can be retrieved via:\nfor disk in reachy.head.neck.disks: print(disk.rot_position) Similarly to access the temperature:\nfor disk in reachy.head.neck.disks: print(disk.temperature) The position are updated at about 100Hz while the temperature is updated at ~1Hz. The temperature is also used internally to trigger the fan inside the Orbita neck.\nLook at specific points in space Move antennas Access the camera "});index.add({'id':15,'href':'/reachy-docs/docs/program-your-robot/pick-and-place/','title':"Pick and Place",'content':"Pick and Place Prepare Reachy workspace With all motors compliant try if you can realise the task you would like the robot to do. We will realise most of the recordings using physical demonstration.\nChoose a good starting point Define a resting position Rest between motions. Real rest position (even turn the motor compliant if possible).\nThe rest position should not force! Static forcing is how the motors heat the fastest.  Record a trajectory from reachy.trajectory import TrajectoryRecorder traj_recorder = TrajectoryRecorder(reachy.right_arm.motors) Compensating gravity  Record the pick trajectory traj_recorder.start() traj_recorder.stop() import numpy as np pick_traj = traj_recorder.trajectories np.savez(\u0026#39;pick-traj.npz\u0026#39;, **pick_traj) Record the place trajectory traj_recorder.start() traj_recorder.stop() import numpy as np place_traj = traj_recorder.trajectories np.savez(\u0026#39;place-traj.npz\u0026#39;, **place_traj) Record the back trajectory traj_recorder.start() traj_recorder.stop() import numpy as np back_traj = traj_recorder.trajectories np.savez(\u0026#39;back-traj.npz\u0026#39;, **back_traj) Putting everything together from reachy.trajectory import TrajectoryPlayer goto_base_position(duration=2) TrajectoryPlayer(reachy, pick_traj).play(wait=True) reachy.right_arm.hand.close() TrajectoryPlayer(reachy, place_traj).play(wait=True) reachy.right_arm.hand.open() TrajectoryPlayer(reachy, back_traj).play(wait=True) goto_resting_position(duration=2) Open \u0026amp; close the gripper Look what you are doing Make it loop "});index.add({'id':16,'href':'/reachy-docs/docs/program-your-robot/ai/','title':"AI \u0026 Coral TPU",'content':"AI \u0026amp; Google Coral TPU "});index.add({'id':17,'href':'/reachy-docs/docs/program-your-robot/python-api/','title':"Python's API",'content':"Python's API The whole Python's API is available here: https://pollen-robotics.github.io/reachy/.\nThe inline documentation can also be accessed directly from Python using instrospection. For instance:\nIn [1]: from reachy import Reachy In [2]: print(Reachy.__doc__) Class representing the connection with the hardware robot. Connect and synchronize with the hardware robot. Args: left_arm (reachy.parts.LeftArm): left arm part if present or None if absent right_arm (reachy.parts.RightArm): right arm part if present or None if absent head (reachy.parts.Head): hrad part if present or None if absent It can be used to monitor real time robot state and to send commands. Mainly a container to hold the different parts of Reachy together. In [3]: help(Reachy) "});index.add({'id':18,'href':'/reachy-docs/docs/tictactoe/','title':"TicTacToe Playground",'content':"TicTacToe Playground The TicTacToe Playground is the first setup we designed with Reachy. We wanted to create a demo to emphasize Reachy's interactivity both with humans and when grasping and moving objects. We also wanted to wrap it as a game as it constrains the interaction (turn-taking, game rules are already known) and well it's fun ðŸ˜„\nThis setup requires a Reachy:\n with a Right Arm to move the pawn and a Head to look and analyze the board.  The robot is attached to a table where the board and the pawn are setup. The dimensions of the table, the board, and the fixation where chosen to permit Reachy to reach and grab easily objects on the table surface.\nThe rest of this section will show you how to run and play the TicTacToe demonstration. It will also give you details on how to customize or adapt some of its key aspects to really fit your needs.\nThe TicTacToe demo The demo lets you play TicTacToe against your Reachy. Reachy plays with the cylinder pawn and you play with the square ones.\nThe demo has been made to be fully autonomous. So, Reachy decides who goes first (it's actually random) and let you know: it will either point to itself or point to you.\nReachy will detects when you place a pawn on the board and automatically plays in response. The game will last until one the player wins or if all pawns have been played. The robot will then react to the end of the game, differently whether it wins or loses. Then, it waits for the board to be cleaned and automatically restarts a new game.\nKey technical aspects This demonstration emphasises several key aspects of the robot:\n It involves complex motions that have been defined, saved and can be re-played (grasping a pawn, place it on a pre-defined location). It uses the camera, vision and machine learning for recognizing the board and analyzing its configuration. It also relies on a simple machine learning algorithm for correctly playing the game itself (what next move should I make).  "});index.add({'id':19,'href':'/reachy-docs/docs/tictactoe/setup-the-demo/','title':"Setup The Demo",'content':"Setup the demo On the ISO provided with the robot, the code to run the TicTacToe playground is already pre-installed. From your Raspberry-Pi board, you can find in the folder ~/dev/reachy-tictactoe.\nInside, you will find:\n the code implementing the gameplay mechanisms, the specific assets: move trajectories, pre-trained vision network a service file to simplify launching  Launch the demo at boot The easiest way to setup the demo is to launch it automatically at startup. Once setup, all you have to do is turn on the robot and after about 30s (basically the Raspberry-Pi boot time), it will directly start playing.\nMake sure you put the robot in a \u0026ldquo;safe\u0026rdquo; position before turning it on. Indeed, as soon as it starts it will try to move to its rest position.\nTODO: image\n We provide a pre-configured service file. So, to make it run automatically at startup, you need to:\n connect to your Raspberry-Pi run the following command:  sudo systemctl enable tictactoe_launcher.service   restart your Raspberry-Pi wait for about 30s and you should see the behavior starting  You only need to do this operation one time. Then, the demo will always start when the robot is turned on.\n If you want to stop the demo, run:  sudo systemctl stop tictactoe_launcher.service   If you want to disable the auto-launch behavior, run:  sudo systemctl disable tictactoe_launcher.service     "});index.add({'id':20,'href':'/reachy-docs/docs/tictactoe/playing-with-reachy/','title':"Playing With Reachy",'content':"Playing with Reachy Unwrapping the demo Controlling the demo As you can see on the diagram, the demo runs in a fully autonomous way. Yet, they are a few way you can control and interact with it.\n  First, the robot will start a game only once the board is cleared. It is up to you to reset the board position and to put back the pawn to their base positions. The robot will only start playing when it's done. When a game is over, a new one is directly restarted. So, at the end of a game clean up the board, and a new game will begin.\n  Then, if something weird happen during a game (like someone cheating, the detection was wrong and so we don't know our current state anymore, etc.), the robot will reset the game. It will perform a shuffle move, where Reachy will overthrow all pawns present on the board. It will then wait for a new game to begin, ie when the board is cleaned again. You can use this behavior to reset the game whenever you want. Simply starts cleaning the board, the robot will be lost, do its shuffle move and start a new game.\n  "});index.add({'id':21,'href':'/reachy-docs/docs/tictactoe/train-your-own-detection/','title':"Train Your Own Vision Detection",'content':"Train your own vision detection Take pictures of the board board, img = tp.reachy.analyze_board() Organize and get your data ready Train your classifier Use your own trained versions "});index.add({'id':22,'href':'/reachy-docs/docs/simulation/','title':"Reachy simulation",'content':"Reachy simulation Based on the Unity engine, we provide our own visualisation and simulation tool for reachy.\n It lets you try to move the simulated robot without any risk of breaking anything. Itâ€™s a good way to learn how to control Reachy without fear. You can try yourself controlling a simulated Reachy before purchasing one, to better understand what it can and cannot do. It can be used extensively and with as many robots as you want for instance to train your machine learning algorithm.  This tool is still rather primitive at the moment, but we are dedicated to improve it and add new features in the coming weeks and month.\nQuickstart What you need So, what do you need to do the same on your own computer?\nFirst, you don't need a real Reachy. You can access the simulator without having the real robot directly here: http://reachy-simu.pollen-robotics.com\nWhat you only need is to install our Pythonâ€™s package. It can be found on GitHub or directly on PyPi. It requires Python 3 and a few classical dependencies (numpy, scipy, etc). This software is the same one that runs on the real robot. We've simply included a specific IO layer that changes the communication with the hardware (motors and sensors) to use a WebSocket communication that interacts with the 3D visualisation.\nYou also need to use a Web browser that is compatible with WebGL, which is the case with all more or less recent browsers. It will also most likely not work on mobile devices, as Unity support is only partial at this time (See https://docs.unity3d.com/Manual/webgl-browsercompatibility.html for details).\nGetting started Both hardware and simulated robot share the same API, to let you switch from simulation to a real robot back and forth easily.\nThey are a few difference that you should take into consideration.\nFirst, when instantiating your robot, you need to specify that you want to use the simulated version. To do that, you simply set all IOs to \u0026lsquo;ws\u0026rsquo; (instead of specifying the USB port on a real Reachy):\nfrom reachy import parts, Reachy r = Reachy( right_arm=parts.RightArm(io=\u0026#39;ws\u0026#39;, hand=\u0026#39;force_gripper\u0026#39;), left_arm=parts.LeftArm(io=\u0026#39;ws\u0026#39;, hand=\u0026#39;force_gripper\u0026#39;), ) This creates a web socket server in the background that connects to the Unity simulation. When opening http://reachy-simu.pollen-robotics.com, click on the \u0026lsquo;Connect\u0026rsquo; button and you should see the connection status going green, meaning both are synced.\nThen, you can run command using Pythonâ€™s API and you should see the visualisation move! For instance:\nr.right_arm.elbow_pitch.goal_position = -80 Navigate in the 3D scene Once in the simulation, you can control the camera and rotate around reachy (with the left click) and move or zoom (with the mouse wheel). Four buttons on the left allow you to show or hide reachy's components if you want to focus on specific parts.\nDifference with controlling a real robot They are still some difference between the real robot and its simulation equivalent. Some will be erased in the future, some are inherent to this kind of problem.\nWhat's currently missing (and should be added soon) At the moment, we do not provide support in the simulation for:\n the head part (you can see it but not control it) the present position can not be read  Those limits should be removed in the next few days.\nExtra sensors, such as the force sensor, are not implemented in the simulation. You can still access them from code to guarantee compatibility but they will return random or nan values. We are currently investigating how they can be implemented within Unity.\nWhat's more complex (but we are working on it!) The current version of the simulation does not rely on physics at all. Motors are teleported to their goal position and no gravity force is applied on the robot.\nWe are planning to better integrate with Unity and take advantage of their new Physics API. This should allow us to have collision detection and interaction with the rest of the environment.\n"});index.add({'id':23,'href':'/reachy-docs/docs/simulation/create-your-own-scene/','title':"Create your own virtual scene for Reachy",'content':"Create your own virtual scene for Reachy We've created a simulated version of our Reachy robot where we simulate its behaviour and motors physics. It comes as a Unity package with everything already setup.\nThis lets you create your own 3D Scene within Unity so you can build your own virtual experimental setup. You can add 3D objects with their physics (a table, objects that can be grasped and moved, etc). You can setup lighting conditions and retrieve what the robot will perceive from its camera. You can also detect collisions, so you can grasp objects, or train the robot to perform some specific moves. You can even add humans avatar to prototype Human-Robot Interactions.\nWhat you need Unity, editor version 2020.1.0b7 or higher. (Get it here) We rely on the new Articulation Body component made for robotics. So it will not work on previous version.\nTo leverage more physical accuracy in your robotics simulation make sure to setup a few parameters in your Project Settings:\n lower the Fixed Timestep, to 0.01s for instance (in the Time Tab) switch the Solver Type to Temporal Gauss Seidel (in the Physics Tab)  Import reachy package You can get the package asset of the reachy simulation on our GitHub deposit: https://github.com/pollen-robotics/reachy-unity-package.\nThe Unity package can be found directly in the release: https://github.com/pollen-robotics/reachy-unity-package/releases/latest.\nTo use it, simply open your project and go to \u0026lsquo;Assets\u0026rsquo; \u0026ndash;\u0026gt; Import package \u0026ndash;\u0026gt; custom package and select our package.\nWhat's inside? Our package is mainly composed of 2 prefabs composed of the meshes and materials for Reachy, plus a few scripts to control it.\nThe Reachy prefab is our robot and the support prefab is simply the support that keeps the robot upright. In the scripts folder, the ReachyController contains the function you will use to control Reachy. A JointController script is associated to each part of Reachy. This script lets you drive the motor as you would with the real robot. Gripper also have a force sensor script that retrieves the current force applied on the gripper.\nWe also provide IO (only a WebSocket for the moment, but other can be easily implemented) scripts which allow you to communicate with the external world. This can be used to control the virtual robot using the same Python's API that we use for the real robot (see section Simulation). This IO let you send command (i-e move Reachy) from an other application or get the current reachy state (i-e reachy's camera image, motors position, gripper force).\nPlay with reachy Reachy is composed of static parts, such as the torso, the left and the right shoulder cap, and of mobile parts such as the head (only the antennas can move for the moment) and the arms.\nYou can directly make those parts move by setting the variables targetPosition in the motors property of the ReachyController component attached to the Reachy Prefab. Each part of an arm is linked to a parent so moving one part will make the children move with it.\nAs, we use the Unity Physics engine, make sure you are in Play Mode before trying to move the robot.  Reachy commands The ReachyCommand script provides high level function to make reachy move and get his current state:\n HandleCommand(SerializableCommands command): Giving a list of motors names and their target position, it will update the robot movements. GetCurrentState(): Get the current state of reachy (composed of the cameras view, and the current position of every motors).  Those functions are for instance called by the IO when you are remotely controlling the robot.\nHandle collisions Each motor use the Articulation Body and colider components, they allow Reachy to interact with other physical objects. According to the Unity Docmentation, \u0026ldquo;a Rigidbody object will be pulled downward by gravity and will react to collisions with incoming objects if the right Collider component is also present\u0026rdquo;. If you want to add other objects to your Scene and have Reachy interact with them, you need to make sure to attach those two components to them. For instance, you can make reachy flip a table by adding a rigidiBody and a collider to a table GameObject and ask reachy to quickly move his arm bottom-up (when close enough to the table).\nArticulation Body Each motor has the component Articulation Body which represents the characteristics of how a specific part of the arm is supposed to move. All motors in Reachy are revolute joints, and they are controlled using the Articulation Drive mechanism.\nWe've preset a few parameters to try to match closely the real behavior of Reachy's motor\n The Lower limit and Upper limit define the same minimum and maximum angle of rotation as the real robot. The stiffness and damping are PD gains of a PID controller. The force limit is the maximum force the joint can apply.  Those parameters were defined by recording moves on the real robot, measure the real trajectory, the backlash and try to reproduce them on the simulation. Yet, depending on your experimental conditions you may need to tune them to get better results.\n"});index.add({'id':24,'href':'/reachy-docs/docs/appendices/','title':"Appendices",'content':""});index.add({'id':25,'href':'/reachy-docs/docs/appendices/install-from-scratch-raspberry/','title':"Install from Scratch on a Raspberry-Pi",'content':"Install from Scratch on a Raspberry-Pi This guide describe how we generate our ISO. Its main use is for us to keep it up-to-date and always accessible for our team. You can freely use it and adapt it to your needs but we don't intend to explain every details here.\nPrepare the Raspberry-Pi  Burn an SD-Card (16Go or more) using Raspbian Buster with desktop - February 2020. Make sure to allow ssh (touch ssh on the boot partition) or directly work from the Pi board using a mouse, keyboard and screen. Power your Raspberry-Pi and connect it to internet via Ethernet (WiFi is not configure at this point). Once booted, connect to it using ssh and the pi login (ssh pi@raspberrypi.local password raspberry)  Raspi-config Once booted and logged in the Raspberry-Pi, we need to run raspi-config to setup few things. So, on the Pi bash run sudo raspi-config (We will give the number/letter corresponding to the entry item for each command below)\n Change user password to reachy: 1 Change hostname to reachy: 2-N1 Turn on WiFi (you can set the country and your own WiFi if you want but this is not necessary) 2-N2-Cancel-Cancel- Turn on the camera 5-P1-Yes Turn on I2C 5-P5-Yes Finish and reboot  Install Reachy needed software  Reconnect ssh pi@reachy.local (password reachy) Create our own work folder mkdir dev and cd dev Clone Reachy software: git clone https://github.com/pollen-robotics/reachy Checkout latest release cd reachyand git checkout tags/v1.1.5 Install atlas for scipy: sudo apt install -y libatlas-base-dev Install deps for opencv: sudo apt install -y libhdf5-dev libhdf5-serial-dev libhdf5-103 libqtgui4 libqtwebkit4 libqt4-test python3-pyqt5 libatlas-base-dev libjasper-dev Install Reachy dependencies using the system python3: pip3 install -r ~/dev/reachy/software/requirements.txt Install Reachy software using the system python3: pip3 install -e ~/dev/reachy/software Get the Access Point and dashboard libraries: cd ~/devand git clone https://github.com/pollen-robotics/RAP Install them: cd ~/dev/RAP/ and sudo bash install.sh (Say Yes both time when asked during install)  Setup dual camera (see https://github.com/ArduCAM/RaspberryPi/tree/master/Multi_Camera_Adapter/Multi_Adapter_Board_2Channel_uc444)\n cd /tmp wget https://project-downloads.drogon.net/wiringpi-latest.deb sudo dpkg -i wiringpi-latest.deb  Setup Coral toolkit (see https://coral.ai/docs/accelerator/get-started/)\n echo \u0026ldquo;deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\u0026rdquo; | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - sudo apt-get update sudo apt-get install libedgetpu1-std pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl cd ~/dev mkdir coral \u0026amp;\u0026amp; cd coral git clone https://github.com/google-coral/tflite.git cd tflite/python/examples/classification bash install_requirements.sh  Install extra stuff  Install IPython, Jupyter, Matplotlib: pip3 install ipython jupyter matplotlib  Check if everything is fine  Reboot Connect to reachy access point (SSID: Reachy-AP) Connect to the dashboard: http://reachy.local  Generate ISO  You will need another Raspberry-Pi with lots of space (64Go) to do that Connect the reachy SD-card to the other Raspberry PI (using an USB adaptor) Create an img using dd (check the disk number using sudo fdisk -l): sudo dd bs=4M if=/dev/sda of=reachy-$(date +%F).img conv=fsync download shrink: wget https://raw.githubusercontent.com/Drewsif/PiShrink/master/pishrink.sh Run sudo bash pishrink.sh -p -z reachy-$(date +%F).img Share the create compressed img!  "});index.add({'id':26,'href':'/reachy-docs/categories/','title':"Categories",'content':""});index.add({'id':27,'href':'/reachy-docs/docs/','title':"Docs",'content':""});index.add({'id':28,'href':'/reachy-docs/','title':"Introduction",'content':"Hello, I'm Reachy! Reachy is an open source interactive robot designed to explore real-world applications!\n  Reachy makes AI \u0026amp; robotics accessible to researchers, innovation professionals and creatives. It comes in different flavors to let you prototype and create your real-world interactive \u0026amp; service applications right away!\u2029This manual will guide you in all the steps to\n getting started and turn it on for the first time, program it to perform your own task, or use one of our playground environments.  You will also find many use examples, specifications and detail implementations. If you are missing some information or want to go deeper and interact with other Reachy's user, don't hesitate to join the discussion on our forum!\n"});index.add({'id':29,'href':'/reachy-docs/tags/','title':"Tags",'content':""});})();